{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from lib.data_handler import trim_cases_by_class\n",
    "from lib.data_handler import balanced_split_list\n",
    "from lib.data_handler import get_data_token_count\n",
    "from lib.data_handler import wv_initialize\n",
    "from lib.data_handler import cnn_tokensToIdx\n",
    "from lib.data_handler import get_list_unique\n",
    "from lib.data_handler import balancedCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from lib import basic_cnn\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "'''\n",
    "valid task names:\n",
    "    gs_behavior_label\n",
    "    gs_organ_label\n",
    "    gs_icd_label\n",
    "    gs_hist_grade_label\n",
    "    gs_lat_label\n",
    "'''\n",
    "# parameters ---------------------------------------------------------------------------\n",
    "task = 'gs_icd_label'\n",
    "#test_prop = .1\n",
    "num_cv = 5\n",
    "val_prop = .25\n",
    "preloadedWV=None\n",
    "min_df = 2  #This means that a term must appear in at least 2 documents to be included in the model's vocabulary. This parameter is useful for removing very rare words or terms that are unlikely to contribute to the model's performance due to their sparse occurrence across the dataset.\n",
    "pretrained_cnn_name = 'pretrained.h5'\n",
    "rand_seed = 3545\n",
    "cnn_seq_len = 1500\n",
    "# n your parameters, cnn_seq_len is set to 1500, meaning that the CNN is designed to process sequences (e.g., sentences, documents, or any other form of sequential data) of up to 1500 elements (such as words, characters, or time steps) in length.\n",
    "#For text processing tasks, this could mean that each document or piece of text is either padded or truncated to ensure it has exactly 1500 tokens (words or characters) before being fed into the CNN model. This uniformity in sequence length is important for batch processing in neural networks, allowing the model to efficiently learn from the data.\n",
    "reverse_seq = True\n",
    "train_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json():\n",
    "    \"\"\"\n",
    "    function to read matched_fd.json as list\n",
    "    \"\"\"\n",
    "    with open('matched_fd.json') as data_file:\n",
    "        data = json.load(data_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_label(task_name,in_data):\n",
    "    \"\"\"\n",
    "    function to get text,labels for valid tasks\n",
    "    \"\"\"\n",
    "    #print(in_data[0])\n",
    "    valid_entries = [x for x in in_data if x[task_name]['match_status']==\"matched\"]\n",
    "    valid_text = [x['doc_raw_text'] for x in valid_entries]\n",
    "    valid_tokens = [cleanText(x) for x in valid_text]\n",
    "    valid_labels = [x[task_name]['match_label'] for x in valid_entries]\n",
    "    return list(zip(valid_tokens,valid_labels)) #it returns 951 valid data\n",
    "\n",
    "def get_task_labels(in_task):\n",
    "    read_data = read_json()\n",
    "    return get_valid_label(in_task,read_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    '''\n",
    "    function to clean text\n",
    "    '''\n",
    "    #replace symbols and tokens\n",
    "    text = re.sub('\\n|\\r', ' ', text)\n",
    "    text = re.sub('o clock', 'oclock', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(p\\.?m\\.?)','pm', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(a\\.?m\\.?)', 'am', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(dr\\.)', 'dr', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('\\*\\*NAME.*[^\\]]\\]', 'nametoken', text)\n",
    "    text = re.sub('\\*\\*DATE.*[^\\]]\\]', 'datetoken', text)\n",
    "    text = re.sub(\"\\?|'\", '', text)\n",
    "    text = re.sub('[^\\w.;:]|_|-', ' ', text)\n",
    "    text = re.sub('[0-9]+\\.[0-9]+','floattoken', text)\n",
    "    text = re.sub('floattokencm','floattoken cm', text)\n",
    "    text = re.sub(' [0-9][0-9][0-9]+ ',' largeint ', text)\n",
    "    text = re.sub('\\.', ' . ', text)\n",
    "    text = re.sub(':', ' : ', text)\n",
    "    text = re.sub(';', ' ; ', text)\n",
    "\n",
    "    #lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #tokenize\n",
    "    text = text.split()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo():\n",
    "    #Initializing a random state with a fixed seed ensures reproducibility. It means that random processes (like data shuffling) are consistent across different runs of the script.\n",
    "    rand_state = np.random.RandomState(rand_seed)\n",
    "    #Calls get_task_labels to read data and labels for the specified task and then applies trim_cases_by_class to remove classes with insufficient data.\n",
    "    #eikhane 951 ta text data and corresponding level ache\n",
    "    data_label_pairs = get_task_labels(task)\n",
    "    \n",
    "    #eikhane shudhu jeishob label er freq shudhu 10 or 10 er upore taderke nise\n",
    "    data_label_pairs = trim_cases_by_class(data_label_pairs)\n",
    "    label_list = [x[1] for x in data_label_pairs] #eikhane data er shudhu label gulake nitese\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(label_list)\n",
    "    cv_list = balancedCV(label_list,num_cv,rand_state)\n",
    "    y_actual,y_pred = [],[]\n",
    "    for this_cv in range(num_cv):\n",
    "        \n",
    "        #train test split (bujhini)\n",
    "        #this cv er upore basis kore on index er value gula train and kon gula test e jabe sheta ber kora hoitese\n",
    "        train_idx = [i for i,cv in enumerate(cv_list) if cv != this_cv]\n",
    "        test_idx = [i for i,cv in enumerate(cv_list) if cv == this_cv]\n",
    "        #train idx or test idx er je index gul ase shei index gular data gula amra niboh\n",
    "        train = [x for i,x in enumerate(data_label_pairs) if i in train_idx]\n",
    "        test = [x for i,x in enumerate(data_label_pairs) if i in test_idx]\n",
    "        \n",
    "        train_label_list = [x[1] for x in train]\n",
    "        train,val = balanced_split_list(train,train_label_list,val_prop)\n",
    "        \n",
    "        vocab_counter = get_data_token_count(train)\n",
    "        \n",
    "        wv_mat, wv_to_idx = wv_initialize(preloadedWV,min_df,vocab_counter,rand_state)\n",
    "        \n",
    "        train_tokens,train_y = list(zip(*train))\n",
    "        \n",
    "        train_x = [cnn_tokensToIdx(x,wv_to_idx,cnn_seq_len,0,reverse_seq) \\\n",
    "        for x in train_tokens] \n",
    "    print(len(train_x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "algo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
