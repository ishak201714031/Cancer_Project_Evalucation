{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from lib.data_handler import trim_cases_by_class\n",
    "from lib.data_handler import balanced_split_list\n",
    "from lib.data_handler import get_data_token_count\n",
    "from lib.data_handler import wv_initialize\n",
    "from lib.data_handler import cnn_tokensToIdx\n",
    "from lib.data_handler import get_list_unique\n",
    "from lib.data_handler import balancedCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from lib import basic_cnn\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "'''\n",
    "valid task names:\n",
    "    gs_behavior_label\n",
    "    gs_organ_label\n",
    "    gs_icd_label\n",
    "    gs_hist_grade_label\n",
    "    gs_lat_label\n",
    "'''\n",
    "# parameters ---------------------------------------------------------------------------\n",
    "task = 'gs_icd_label'\n",
    "#test_prop = .1\n",
    "num_cv = 1\n",
    "val_prop = .25\n",
    "preloadedWV=None\n",
    "min_df = 2  #This means that a term must appear in at least 2 documents to be included in the model's vocabulary. This parameter is useful for removing very rare words or terms that are unlikely to contribute to the model's performance due to their sparse occurrence across the dataset.\n",
    "pretrained_cnn_name = 'pretrained.h5'\n",
    "rand_seed = 3545\n",
    "cnn_seq_len = 1500\n",
    "# n your parameters, cnn_seq_len is set to 1500, meaning that the CNN is designed to process sequences (e.g., sentences, documents, or any other form of sequential data) of up to 1500 elements (such as words, characters, or time steps) in length.\n",
    "#For text processing tasks, this could mean that each document or piece of text is either padded or truncated to ensure it has exactly 1500 tokens (words or characters) before being fed into the CNN model. This uniformity in sequence length is important for batch processing in neural networks, allowing the model to efficiently learn from the data.\n",
    "reverse_seq = True\n",
    "train_epochs = 50\n",
    "\n",
    "def read_json():\n",
    "    \"\"\"\n",
    "    function to read matched_fd.json as list\n",
    "    \"\"\"\n",
    "    with open('raw_last_150_items.json') as data_file:\n",
    "        data = json.load(data_file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_valid_label(task_name,in_data):\n",
    "    \"\"\"\n",
    "    function to get text,labels for valid tasks\n",
    "    \"\"\"\n",
    "    #print(in_data[0])\n",
    "    # valid_entries = [x for x in in_data if x[task_name]['match_status']==\"matched\"]\n",
    "    \n",
    "    valid_entries = in_data\n",
    "    valid_text = [x['doc_raw_text'] for x in valid_entries]\n",
    "    valid_tokens = [cleanText(x) for x in valid_text]\n",
    "    valid_labels = [x[task_name]['match_label'] for x in valid_entries]\n",
    "    return list(zip(valid_tokens,valid_labels)) #it returns 951 valid data\n",
    "\n",
    "def get_task_labels(in_task):\n",
    "    read_data = read_json()\n",
    "    return get_valid_label(in_task,read_data)\n",
    "\n",
    "def cleanText(text):\n",
    "    '''\n",
    "    function to clean text\n",
    "    '''\n",
    "    #replace symbols and tokens\n",
    "    text = re.sub('\\n|\\r', ' ', text)\n",
    "    text = re.sub('o clock', 'oclock', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(p\\.?m\\.?)','pm', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(a\\.?m\\.?)', 'am', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'(dr\\.)', 'dr', text, flags=re.IGNORECASE)\n",
    "    text = re.sub('\\*\\*NAME.*[^\\]]\\]', 'nametoken', text)\n",
    "    text = re.sub('\\*\\*DATE.*[^\\]]\\]', 'datetoken', text)\n",
    "    text = re.sub(\"\\?|'\", '', text)\n",
    "    text = re.sub('[^\\w.;:]|_|-', ' ', text)\n",
    "    text = re.sub('[0-9]+\\.[0-9]+','floattoken', text)\n",
    "    text = re.sub('floattokencm','floattoken cm', text)\n",
    "    text = re.sub(' [0-9][0-9][0-9]+ ',' largeint ', text)\n",
    "    text = re.sub('\\.', ' . ', text)\n",
    "    text = re.sub(':', ' : ', text)\n",
    "    text = re.sub(';', ' ; ', text)\n",
    "\n",
    "    #lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #tokenize\n",
    "    text = text.split()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def algo():\n",
    "    \n",
    "    rand_state = np.random.RandomState(rand_seed)\n",
    "    data_label_pairs = get_task_labels(task)\n",
    "    # label_list = [x[1] for x in data_label_pairs]\n",
    "    # label_encoder = LabelEncoder()\n",
    "    # label_encoder.fit(label_list)\n",
    "    \n",
    "    # cv_list = balancedCV(label_list, num_cv, rand_state)\n",
    "\n",
    "    y_actual, y_pred = [], []\n",
    "    # for this_cv in range(num_cv):\n",
    "    # train_idx = [i for i, cv in enumerate(cv_list) if cv != this_cv]\n",
    "    # test_idx = [i for i, cv in enumerate(cv_list) if cv == this_cv or cv != this_cv]\n",
    "\n",
    "    # train = [x for i, x in enumerate(data_label_pairs) if i in train_idx]\n",
    "    test = data_label_pairs\n",
    "    \n",
    "    # with open('test_data.pkl', 'rb') as file:\n",
    "    #     test = pickle.load(file)\n",
    "\n",
    "    print(len(test))\n",
    "    print(test[0])\n",
    "    \n",
    "    with open('vocab.pkl', 'rb') as handle:\n",
    "        vocab_counter = pickle.load(handle)\n",
    "    \n",
    "    wv_mat, wv_to_idx = wv_initialize(preloadedWV,min_df,vocab_counter,rand_state)  \n",
    "    \n",
    "    test_tokens,test_y = list(zip(*test))\n",
    "    test_x = [cnn_tokensToIdx(x,wv_to_idx,cnn_seq_len,0,reverse_seq) \\\n",
    "             for x in test_tokens]\n",
    "        \n",
    "    # print(test_x[0])\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "(['clinical', 'history', ':', 'atypical', 'lymph', 'node', 'suspicious', 'for', 'adenocarcinoma', '.', 'ap', 'speci', 'gross', 'and', 'microscopic', 'pathologic', 'diagnosis', ':', 'a', '.', 'lung', 'right', 'upper', 'lobe', 'wedge', ':', 'invasive', 'non', 'small', 'cell', 'carcinoma', '1', '.', 'histologic', 'type', ':', 'invasive', 'adenocarcinoma', 'with', 'a', 'lepidic', 'pattern', '.', '2', '.', 'grade', ':', 'well', 'differentiated', '.', '3', '.', 'tumor', 'size', ':', 'tumor', 'measures', 'floattoken', 'x', 'floattoken', 'x', 'floattoken', 'cm', '.', '4', '.', 'margins', 'of', 'resection', 'and', 'extent', ':', 'a', '.', 'the', 'specimen', 'is', 'intact', '.', 'b', '.', 'the', 'tumor', 'is', 'unifocal', '.', 'c', '.', 'no', 'visceral', 'pleural', 'invasion', 'is', 'identified', '.', 'd', '.', 'the', 'tumor', 'is', 'confined', 'to', 'the', 'lung', '.', 'e', '.', 'the', 'wedge', 'margin', 'is', 'free', 'of', 'tumor', '.', 'f', '.', 'the', 'tumor', 'comes', 'within', 'floattoken', 'cm', 'of', 'the', 'wedge', 'margin', '.', 'g', '.', 'no', 'lymphovascular', 'invasion', 'is', 'identified', '.', '5', '.', 'additional', 'findings', ':', 'calcified', 'granuloma', 'and', 'several', 'non', 'caseating', 'granuloma', '.', 'b', '.', 'right', 'lower', 'lobe', 'fissure', 'nodule', ':', 'lung', 'with', 'areas', 'of', 'non', 'caseating', 'granuloma', 'c', '.', 'inferior', 'pulmonary', 'ligament', 'node', ':', 'benign', 'node', 'with', 'numerous', 'non', 'caseating', 'granuloma', 'd', '.', 'right', 'upper', 'lobe', 'lymph', 'node', ':', 'benign', 'node', 'with', 'numerous', 'non', 'caseating', 'granuloma', 'e', '.', 'lung', 'right', 'upper', 'lobe', 'lobectomy', ':', 'benign', 'lobectomy', 'specimen', 'with', 'no', 'evidence', 'of', 'residual', 'tumor', 'one', 'benign', 'bronchial', 'margin', 'with', 'lymph', 'nodes', 'with', 'granuloma', 'four', 'benign', 'parabronchial', 'lymph', 'nodes', 'with', 'non', 'caseating', 'granuloma', 'benign', 'vascular', 'margin', 'f', '.', 'right', 'lower', 'paratracheal', 'lymph', 'node', ':', 'three', 'benign', 'lymph', 'nodes', 'with', 'extensive', 'non', 'caseating', 'granuloma', 'g', '.', 'parabronchial', 'lymph', 'node', ':', 'benign', 'lymph', 'node', 'with', 'non', 'caseating', 'granuloma', 'comment', ':', 'numerous', 'non', 'caseating', 'granuloma', 'are', 'identified', 'in', 'several', 'lymph', 'nodes', 'and', 'there', 'are', 'also', 'several', 'granuloma', 'in', 'lungs', 'primarily', 'in', 'a', 'parabronchial', 'distribution', '.', 'some', 'possibilities', 'could', 'include', 'infection', 'sarcoid', 'or', 'hypersensitivity', 'pneumonitis', '.', 'elastic', 'stains', 'were', 'performed', 'on', 'specimen', 'a', 'and', 'there', 'is', 'no', 'visceral', 'pleural', 'invasion', '.', 'afb', 'and', 'gms', 'stains', 'were', 'performed', 'and', 'no', 'acid', 'fast', 'bacilli', 'or', 'fungal', 'organisms', 'are', 'identified', '.', 'dr', 'name', 'zzz', 'has', 'seen', 'this', 'case', 'in', 'an', 'internal', 'consult', 'and', 'concurs', '.', 'pathology', 'id', 'num', 'initials', 'name', 'yyy', ':', 'xxx', 'm', '.', 'www', 'm', '.', 'd', '.', 'pathologist', 'electronically', 'signed', 'datetoken', ':', 'benign', 'bronchial', 'margin', '.', 'the', 'frozen', 'section', 'remnant', 'is', 'submitted', 'in', 'e1', '.', 'the', 'specimen', 'weighs', 'floattoken', 'grams', '.', 'the', 'bronchial', 'margin', 'is', 'submitted', 'in', 'e1', '.', 'the', 'vessel', 'margin', 'in', 'e3', '.', 'sections', 'around', 'the', 'bronchial', 'margin', 'in', 'e2', '.', 'potential', 'parabronchial', 'lymph', 'nodes', 'in', 'e4', '.', 'no', 'additional', 'lesions', 'are', 'identified', '.', 'sections', 'of', 'lung', 'tissue', 'in', 'e5', 'and', 'e6', '.', 'specimen', 'f', 'the', 'specimen', 'is', 'received', 'in', 'formalin', 'labeled', 'with', 'the', 'patients', 'name', 'and', 'designated', 'right', 'lower', 'paratracheal', 'lymph', 'node', '.', 'the', 'specimen', 'consists', 'of', 'fragments', 'of', 'lymph', 'node', 'measuring', 'in', 'aggregate', 'floattoken', 'x', 'floattoken', 'x', 'floattoken', 'cm', '.', 'the', 'specimen', 'is', 'submitted', 'en', 'toto', 'in', 'a', 'single', 'cassette', '.', 'specimen', 'g', 'the', 'specimen', 'is', 'received', 'in', 'formalin', 'labeled', 'with', 'the', 'patients', 'name', 'and', 'designated', 'parabronchial', 'lymph', 'node', '.', 'the', 'specimen', 'consists', 'of', 'two', 'tan', 'lymph', 'nodes', 'measuring', 'in', 'aggregate', 'floattoken', 'x', 'floattoken', 'x', 'floattoken', 'cm', '.', 'the', 'specimen', 'is', 'submitted', 'en', 'toto', 'in', 'a', 'single', 'cassette', '.', 'initials', 'total', 'blocks', 'submitted', ':', '12', 'lung', ':', 'resection', 'specimen', 'type', ':', 'lobectomy', 'laterality', ':', 'right', 'tumor', 'site', ':', 'upper', 'lobe', 'tumor', 'size', ':', 'greatest', 'dimension', ':', 'floattoken', 'cm', 'x', 'floattoken', 'cm', 'x', 'floattoken', 'cm', 'histologic', 'type', ':', 'adenocarcinoma', 'other', 'variant', 'adenocarcinoma', 'with', 'a', 'lepidic', 'pattern', 'histologic', 'grade', ':', 'g1', 'visceral', 'pleural', 'invasion', ':', 'not', 'identified', 'pathologic', 'staging', 'ptnm', 'primary', 'tumor', 'pt', ':', 'pt1a', 'regional', 'lymph', 'nodes', 'pn', ':', 'pn0', 'number', 'examined', ':', '10', 'number', 'involved', ':', '0', 'distant', 'metastasis', 'pm', ':', 'pmx', 'margins', ':', 'margins', 'uninvolved', 'by', 'invasive', 'carcinoma', 'distance', 'of', 'invasive', 'carcinoma', 'from', 'closest', 'margin', ':', '11mm', 'margin', ':', 'wedge', 'margin', 'in', 'part', 'a', 'but', 'a', 'lobectomy', 'followed', 'in', 'e', 'so', 'this', 'is', 'much', 'greater', 'direct', 'extension', 'of', 'tumor', ':', 'none', 'identified', 'venous', 'large', 'vessel', 'invasion', 'v', ':', 'absent', 'arterial', 'large', 'vessel', 'invasion', ':', 'absent', 'lymphatic', 'small', 'vessel', 'invasion', 'l', ':', 'absent', 'specimen', 's', ':', 'a', 'right', 'upper', 'lung', 'wedge', 'specimen', 's', ':', 'b', 'right', 'lower', 'lobe', 'fissure', 'nodule', 'specimen', 's', ':', 'c', 'inferior', 'pulmonary', 'ligament', 'node', 'specimen', 's', ':', 'd', 'right', 'upper', 'lobe', 'lymph', 'node', 'specimen', 's', ':', 'e', 'lung', 'rul', 'specimen', 's', ':', 'f', 'right', 'lower', 'paratracheal', 'lymph', 'node', 'specimen', 's', ':', 'g', 'parabronchial', 'lymph', 'node'], 'c34.1')\n"
     ]
    }
   ],
   "source": [
    "algo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
